{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/BaBa/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen  [0.38450292397660846, 0.34736842105263133, 0.29671052631578898, 0.39858292730149203, 0.37493154716009242, 0.35182030309765328]\n",
      "dis  [0.016812865497076016, 0.015789473684210534, 0.015789473684210516, 0.017591277443472445, 0.016836162500448507, 0.016822157853132725]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dc30348039e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-dc30348039e4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m                     _ = sess.run(discriminator.d_updates,\n\u001b[1;32m    186\u001b[0m                                  feed_dict={discriminator.u: input_user, discriminator.i: input_item,\n\u001b[0;32m--> 187\u001b[0;31m                                             discriminator.label: input_label})\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;31m# Train G\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/BaBa/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/BaBa/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/BaBa/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/BaBa/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/BaBa/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from dis_model import DIS\n",
    "from gen_model import GEN\n",
    "import pickle\n",
    "import numpy as np\n",
    "import utils as ut\n",
    "import multiprocessing\n",
    "#########################################################################################\n",
    "# FeedDic\n",
    "#########################################################################################\n",
    "\n",
    "def placeholder_inputs(batch_size):\n",
    "  \"\"\"Generate placeholder variables to represent the input tensors.\n",
    "  These placeholders are used as inputs by the rest of the model building\n",
    "  code and will be fed from the downloaded data in the .run() loop, below.\n",
    "  Args:\n",
    "    batch_size: The batch size will be baked into both placeholders.\n",
    "  Returns:\n",
    "    images_placeholder: Images placeholder.\n",
    "    labels_placeholder: Labels placeholder.\n",
    "  \"\"\"\n",
    "  # Note that the shapes of the placeholders match the shapes of the full\n",
    "  # image and label tensors, except the first dimension is now batch_size\n",
    "  # rather than the full size of the train or test data sets.\n",
    "  images_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                         mnist.IMAGE_PIXELS))\n",
    "  labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    return images_placeholder, labels_placeholder\n",
    "\n",
    "def fill_feed_dict(data_set, images_pl, labels_pl):\n",
    "  \"\"\"Fills the feed_dict for training the given step.\n",
    "  A feed_dict takes the form of:\n",
    "  feed_dict = {\n",
    "      <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "      ....\n",
    "  }\n",
    "  Args:\n",
    "    data_set: The set of images and labels, from input_data.read_data_sets()\n",
    "    images_pl: The images placeholder, from placeholder_inputs().\n",
    "    labels_pl: The labels placeholder, from placeholder_inputs().\n",
    "  Returns:\n",
    "    feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "  \"\"\"\n",
    "  # Create the feed_dict for the placeholders filled with the next\n",
    "    # batch size examples.\n",
    "  images_feed, labels_feed = data_set.next_batch(FLAGS.batch_size,\n",
    "                                                 FLAGS.fake_data)\n",
    "  feed_dict = {\n",
    "      images_pl: images_feed,\n",
    "      labels_pl: labels_feed,\n",
    "  }\n",
    "  return feed_dict\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "                                               \n",
    "feed_dict2 = {\n",
    "    tf.placeholder(tf.int32): input_user,\n",
    "    tf.placeholder(tf.int32): input_item,\n",
    "    tf.placeholder(tf.float32):input_label,\n",
    "}\n",
    "\n",
    "images_feed, labels_feed = data_set.next_batch(FLAGS.batch_size,\n",
    "                                               FLAGS.fake_data)\n",
    "\n",
    "fill_feed_dict(data_sets.train,\n",
    "                               images_placeholder,\n",
    "                               labels_placeholder)\n",
    "\n",
    "#########################################################################################\n",
    "# Hyper-parameters\n",
    "#########################################################################################\n",
    "EMB_DIM = 5\n",
    "USER_NUM = 943\n",
    "ITEM_NUM = 1683\n",
    "BATCH_SIZE = 16\n",
    "INIT_DELTA = 0.05\n",
    "\n",
    "all_items = set(range(ITEM_NUM))\n",
    "workdir = 'ml-100k/'\n",
    "DIS_TRAIN_FILE = workdir + 'dis-train.txt'\n",
    "\n",
    "#########################################################################################\n",
    "# Load data\n",
    "#########################################################################################\n",
    "user_pos_train = {}\n",
    "with open(workdir + 'movielens-100k-train.txt')as fin:\n",
    "    for line in fin:\n",
    "        line = line.split()\n",
    "        uid = int(line[0])\n",
    "        iid = int(line[1])\n",
    "        r = float(line[2])\n",
    "        if r > 3.99:\n",
    "            if uid in user_pos_train:\n",
    "                user_pos_train[uid].append(iid)\n",
    "            else:\n",
    "                user_pos_train[uid] = [iid]\n",
    "\n",
    "user_pos_test = {}\n",
    "with open(workdir + 'movielens-100k-test.txt')as fin:\n",
    "    for line in fin:\n",
    "        line = line.split()\n",
    "        uid = int(line[0])\n",
    "        iid = int(line[1])\n",
    "        r = float(line[2])\n",
    "        if r > 3.99:\n",
    "            if uid in user_pos_test:\n",
    "                user_pos_test[uid].append(iid)\n",
    "            else:\n",
    "                user_pos_test[uid] = [iid]\n",
    "\n",
    "all_users = list(user_pos_train.keys())\n",
    "all_users.sort()\n",
    "\n",
    "\n",
    "def dcg_at_k(r, k):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / dcg_max\n",
    "\n",
    "\n",
    "def simple_test_one_user(x):\n",
    "    rating = x[0]\n",
    "    u = x[1]\n",
    "\n",
    "    test_items = list(all_items - set(user_pos_train[u]))\n",
    "    item_score = []\n",
    "    for i in test_items:\n",
    "        item_score.append((i, rating[i]))\n",
    "\n",
    "    item_score = sorted(item_score, key=lambda x: x[1])\n",
    "    item_score.reverse()\n",
    "    item_sort = [x[0] for x in item_score]\n",
    "\n",
    "    r = []\n",
    "    for i in item_sort:\n",
    "        if i in user_pos_test[u]:\n",
    "            r.append(1)\n",
    "        else:\n",
    "            r.append(0)\n",
    "\n",
    "    p_3 = np.mean(r[:3])\n",
    "    p_5 = np.mean(r[:5])\n",
    "    p_10 = np.mean(r[:10])\n",
    "    ndcg_3 = ndcg_at_k(r, 3)\n",
    "    ndcg_5 = ndcg_at_k(r, 5)\n",
    "    ndcg_10 = ndcg_at_k(r, 10)\n",
    "\n",
    "    return np.array([p_3, p_5, p_10, ndcg_3, ndcg_5, ndcg_10])\n",
    "\n",
    "\n",
    "def simple_test(sess, model):\n",
    "    result = np.array([0.] * 6)\n",
    "    pool = multiprocessing.Pool(cores)\n",
    "    batch_size = 128\n",
    "    test_users = list(user_pos_test.keys())\n",
    "    test_user_num = len(test_users)\n",
    "    index = 0\n",
    "    while True:\n",
    "        if index >= test_user_num:\n",
    "            break\n",
    "        user_batch = test_users[index:index + batch_size]\n",
    "        index += batch_size\n",
    "\n",
    "        user_batch_rating = sess.run(model.all_rating, {model.u: user_batch})\n",
    "        user_batch_rating_uid = list(zip(user_batch_rating, user_batch))\n",
    "        batch_result = pool.map(simple_test_one_user, user_batch_rating_uid)\n",
    "        for re in batch_result:\n",
    "            result += re\n",
    "\n",
    "    pool.close()\n",
    "    ret = result / test_user_num\n",
    "    ret = list(ret)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def generate_for_d(sess, model, filename):\n",
    "    data = []\n",
    "    for u in user_pos_train:\n",
    "        pos = user_pos_train[u]\n",
    "\n",
    "        rating = sess.run(model.all_rating, {model.u: [u]})\n",
    "        rating = np.array(rating[0]) / 0.2  # Temperature\n",
    "        exp_rating = np.exp(rating)\n",
    "        prob = exp_rating / np.sum(exp_rating)\n",
    "\n",
    "        neg = np.random.choice(np.arange(ITEM_NUM), size=len(pos), p=prob)\n",
    "        for i in range(len(pos)):\n",
    "            data.append(str(u) + '\\t' + str(pos[i]) + '\\t' + str(neg[i]))\n",
    "\n",
    "    with open(filename, 'w')as fout:\n",
    "        fout.write('\\n'.join(data))\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"load model...\")\n",
    "    model_pickle = open(workdir + \"model_dns_ori.pkl\", 'rb')\n",
    "    param = pickle.load(model_pickle, encoding='latin1')\n",
    "#    param = pickle.load(open(workdir + \"model_dns_ori.pkl\"))\n",
    "    generator = GEN(ITEM_NUM, USER_NUM, EMB_DIM, lamda=0.0 / BATCH_SIZE, param=param, initdelta=INIT_DELTA,\n",
    "                    learning_rate=0.001)\n",
    "    discriminator = DIS(ITEM_NUM, USER_NUM, EMB_DIM, lamda=0.1 / BATCH_SIZE, param=None, initdelta=INIT_DELTA,\n",
    "                        learning_rate=0.001)\n",
    "\n",
    "    #config = tf.ConfigProto()\n",
    "    #config.gpu_options.allow_growth = True\n",
    "    #sess = tf.Session(config=config)\n",
    "    config=tf.ConfigProto(log_device_placement=True) #ali changed\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "    sess = tf.Session(config=config) #ali changed\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    print(\"gen \", simple_test(sess, generator))\n",
    "    print(\"dis \", simple_test(sess, discriminator))\n",
    "\n",
    "    dis_log = open(workdir + 'dis_log.txt', 'w')\n",
    "    gen_log = open(workdir + 'gen_log.txt', 'w')\n",
    "\n",
    "    # minimax training\n",
    "    best = 0.\n",
    "    for epoch in range(15):\n",
    "        if epoch >= 0:\n",
    "            for d_epoch in range(100):\n",
    "                if d_epoch % 5 == 0:\n",
    "                    generate_for_d(sess, generator, DIS_TRAIN_FILE)\n",
    "                    train_size = ut.file_len(DIS_TRAIN_FILE)\n",
    "                index = 1\n",
    "                while True:\n",
    "                    if index > train_size:\n",
    "                        break\n",
    "                    if index + BATCH_SIZE <= train_size + 1:\n",
    "                        input_user, input_item, input_label = ut.get_batch_data(DIS_TRAIN_FILE, index, BATCH_SIZE)\n",
    "                    else:\n",
    "                        input_user, input_item, input_label = ut.get_batch_data(DIS_TRAIN_FILE, index,\n",
    "                                                                                train_size - index + 1)\n",
    "                    index += BATCH_SIZE\n",
    "\n",
    "                    _ = sess.run(discriminator.d_updates,\n",
    "                                 feed_dict={discriminator.u: input_user, discriminator.i: input_item,\n",
    "                                            discriminator.label: input_label})\n",
    "\n",
    "            # Train G\n",
    "            for g_epoch in range(50):  # 50\n",
    "                for u in user_pos_train:\n",
    "                    sample_lambda = 0.2\n",
    "                    pos = user_pos_train[u]\n",
    "\n",
    "                    rating = sess.run(generator.all_logits, {generator.u: u})\n",
    "                    exp_rating = np.exp(rating)\n",
    "                    prob = exp_rating / np.sum(exp_rating)  # prob is generator distribution p_\\theta\n",
    "\n",
    "                    pn = (1 - sample_lambda) * prob\n",
    "                    pn[pos] += sample_lambda * 1.0 / len(pos)\n",
    "                    # Now, pn is the Pn in importance sampling, prob is generator distribution p_\\theta\n",
    "\n",
    "                    sample = np.random.choice(np.arange(ITEM_NUM), 2 * len(pos), p=pn)\n",
    "                    ###########################################################################\n",
    "                    # Get reward and adapt it with importance sampling\n",
    "                    ###########################################################################\n",
    "                    reward = sess.run(discriminator.reward, {discriminator.u: u, discriminator.i: sample})\n",
    "                    reward = reward * prob[sample] / pn[sample]\n",
    "                    ###########################################################################\n",
    "                    # Update G\n",
    "                    ###########################################################################\n",
    "                    _ = sess.run(generator.gan_updates,\n",
    "                                 {generator.u: u, generator.i: sample, generator.reward: reward})\n",
    "\n",
    "                result = simple_test(sess, generator)\n",
    "                print(\"epoch \", epoch, \"gen: \", result)\n",
    "                buf = '\\t'.join([str(x) for x in result])\n",
    "                gen_log.write(str(epoch) + '\\t' + buf + '\\n')\n",
    "                gen_log.flush()\n",
    "\n",
    "                p_5 = result[1]\n",
    "                if p_5 > best:\n",
    "                    print('best: ', result)\n",
    "                    best = p_5\n",
    "                    generator.save_model(sess, \"ml-100k/gan_generator.pkl\")\n",
    "\n",
    "    gen_log.close()\n",
    "    dis_log.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
